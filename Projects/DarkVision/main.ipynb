{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastcore.basics import *\n",
    "from fastcore.parallel import *\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, precision_score, recall_score, f1_score, accuracy_score, balanced_accuracy_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_parquet('dataset/cicdarknet.parquet')\n",
    "\n",
    "target = 'Label'\n",
    "df.drop(columns=['Label.1'], inplace=True)  # Remove redundant column\n",
    "\n",
    "tor = df.loc[(df.Label == 'Tor') | (df.Label == 'Non-Tor')].copy(deep=True)\n",
    "\n",
    "tor['Label'] = tor['Label'].astype(dtype='object')\n",
    "tor.loc[tor['Label'] == 'Tor', 'Label'] = 1\n",
    "tor.loc[tor['Label'] == 'Non-Tor', 'Label'] = 0\n",
    "tor['Label'] = tor['Label'].astype(dtype=np.int32)\n",
    "\n",
    "# Split data into training and testing sets\n",
    "training_set = tor.sample(frac=0.2, replace=False, random_state=42)\n",
    "testing_set = tor.drop(index=training_set.index)\n",
    "X_train, y_train = training_set.drop(columns=[target]), training_set[target]\n",
    "X_test, y_test = testing_set.drop(columns=[target]), testing_set[target]\n",
    "\n",
    "# Evaluate features using OneR\n",
    "conts = list(df.columns.difference([target]).values)\n",
    "\n",
    "def evaluate_one_feature(feature, metric=roc_auc_score):    \n",
    "    model = DecisionTreeClassifier(max_depth=1, criterion='gini', class_weight='balanced')    \n",
    "    model.fit(X_train[[feature]], y_train)    \n",
    "    preds = model.predict(X_test[[feature]])\n",
    "    preds_train = model.predict(X_train[[feature]])    \n",
    "    score = round(metric(y_test, preds), 4)\n",
    "    if score > 0.5:\n",
    "        return [feature, score, model, preds, preds_train]\n",
    "    return [feature, score, None, [], []]\n",
    "\n",
    "# Run evaluation\n",
    "results = parallel(f=evaluate_one_feature, items=conts, n_workers=os.cpu_count(), threadpool=False, progress=True)\n",
    "result_df = pd.DataFrame(results, columns=['feature', 'roc_auc_score', 'fitted_models', 'predictions', 'preds_train']).sort_values(by='roc_auc_score', ascending=False)\n",
    "\n",
    "# Select useful features\n",
    "useful_features = result_df.loc[result_df['roc_auc_score'] > 0.5]\n",
    "ensemble_preds = np.mean(np.vstack(useful_features['predictions'].to_numpy()), axis=0)\n",
    "ensemble_preds_train = np.mean(np.vstack(useful_features['preds_train'].to_numpy()), axis=0)\n",
    "\n",
    "# Determine best threshold\n",
    "fpr, tpr, thresholds = roc_curve(y_train, ensemble_preds_train)\n",
    "J = tpr - fpr\n",
    "ix = np.argmax(J)\n",
    "best_thresh = thresholds[ix]\n",
    "\n",
    "# Evaluate model\n",
    "print(\"Best threshold\", best_thresh)\n",
    "print(\"ROC-AUC\", round(roc_auc_score(y_true=y_test, y_score=ensemble_preds), 4))\n",
    "print(\"Precision\", round(precision_score(y_true=y_test, y_pred=np.where(ensemble_preds >= best_thresh, 1, 0)), 4))\n",
    "print(\"Recall\", round(recall_score(y_true=y_test, y_pred=np.where(ensemble_preds >= best_thresh, 1, 0)), 4))\n",
    "print(\"F1 Score\", round(f1_score(y_true=y_test, y_pred=np.where(ensemble_preds >= best_thresh, 1, 0)), 4))\n",
    "print(\"Accuracy\", round(accuracy_score(y_true=y_test, y_pred=np.where(ensemble_preds >= best_thresh, 1, 0)), 4))\n",
    "print(\"Balanced Accuracy\", round(balanced_accuracy_score(y_true=y_test, y_pred=np.where(ensemble_preds >= best_thresh, 1, 0)), 4))\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_true=y_test, y_pred=np.where(ensemble_preds >= best_thresh, 1, 0))\n",
    "ConfusionMatrixDisplay(cm).plot()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
